{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "TRAINING_FILE_PATH = \"Training Data Scrubbing Names 3.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Enhancing the model with States, Cities and company abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of U.S. states and their abbreviations\n",
    "us_states = [\n",
    "    'Alabama', 'AL', 'Alaska', 'AK', 'Arizona', 'AZ', 'Arkansas', 'AR', 'California', 'CA', 'Colorado', 'CO',\n",
    "    'Connecticut', 'CT', 'Delaware', 'DE', 'Florida', 'FL', 'Georgia', 'GA', 'Hawaii', 'HI', 'Idaho', 'ID',\n",
    "    'Illinois', 'IL', 'Indiana', 'IN', 'Iowa', 'IA', 'Kansas', 'KS', 'Kentucky', 'KY', 'Louisiana', 'LA',\n",
    "    'Maine', 'ME', 'Maryland', 'MD', 'Massachusetts', 'MA', 'Michigan', 'MI', 'Minnesota', 'MN', 'Mississippi', 'MS',\n",
    "    'Missouri', 'MO', 'Montana', 'MT', 'Nebraska', 'NE', 'Nevada', 'NV', 'New Hampshire', 'NH', 'New Jersey', 'NJ',\n",
    "    'New Mexico', 'NM', 'New York', 'NY', 'North Carolina', 'NC', 'North Dakota', 'ND', 'Ohio', 'OH', 'Oklahoma', 'OK',\n",
    "    'Oregon', 'OR', 'Pennsylvania', 'PA', 'Rhode Island', 'RI', 'South Carolina', 'SC', 'South Dakota', 'SD', 'Tennessee', 'TN',\n",
    "    'Texas', 'TX', 'Utah', 'UT', 'Vermont', 'VT', 'Virginia', 'VA', 'Washington', 'WA', 'West Virginia', 'WV', 'Wisconsin', 'WI', 'Wyoming', 'WY'\n",
    "]\n",
    "\n",
    "# List of major U.S. cities\n",
    "us_cities = [\n",
    "    'New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia', 'San Antonio', 'San Diego', 'Dallas',\n",
    "    'San Jose', 'Austin', 'Jacksonville', 'Fort Worth', 'Columbus', 'San Francisco', 'Charlotte', 'Indianapolis', 'Seattle',\n",
    "    'Denver', 'Washington', 'Boston', 'El Paso', 'Nashville', 'Detroit', 'Oklahoma City', 'Portland', 'Las Vegas',\n",
    "    'Memphis', 'Louisville', 'Baltimore', 'Milwaukee', 'Albuquerque', 'Tucson', 'Fresno', 'Sacramento', 'Kansas City',\n",
    "    'Atlanta', 'Miami', 'Colorado Springs', 'Raleigh', 'Omaha', 'Long Beach', 'Virginia Beach', 'Oakland', 'Minneapolis',\n",
    "    'Tulsa', 'Arlington', 'Tampa', 'New Orleans', 'Wichita'\n",
    "]\n",
    "\n",
    "# List of common company-related abbreviations\n",
    "company_abbr = [\n",
    "    'Co', 'Inc', 'LLC', 'Ltd', 'Corp', 'Pty', 'PLC', 'GmbH', 'S.A.', 'S.A.S.', 'AG', 'N.V.', 'B.V.', 'K.K.', 'S.R.L.', 'P.C.',\n",
    "    'C.A.', 'd.o.o.', 'P.L.C.', 'S.p.A.', 'A.G.', 'a.s.', 'OÜ', 'Oy', 'ApS', 's.r.o.', 'S.A.B.', 'S.L.', 'AB', 'CO', 'BK'\n",
    "]\n",
    "\n",
    "# Combine all keywords\n",
    "company_keywords = [\n",
    "    'OF', 'SERVICES', 'COUNTY', 'DISTRIBUTOR', 'PRODUCTS', 'COUNTRY CLUB', 'PLLC', 'PRIVATE',\n",
    "    'COMPANY', 'RETIREMENT', 'DEVELOPMENT', 'HOA', 'AUTHORITY', 'CONF', 'CONFERENCE',\n",
    "    'CONSTRUCTION', 'AFFORDABLE', 'HOUSING', 'MID COAST', 'ESTATE', 'REHABILITATION', 'GARDENS',\n",
    "    'WELLNESS', 'STATE', 'LAND', 'MEMBERSHIP', 'COOPERATIVE', 'CORPORATION',\n",
    "    'INDEPENDENT', 'CHURCH', 'ENTERPRISES', 'ACCOUNTING', 'INVESTMENTS', 'OWNERS', 'AIRPORT',\n",
    "    'MAINTENANCE', 'ASSOCIATION', 'REALTY', 'FOUNDATION', 'CONSULTANTS', 'ASSOCIATES',\n",
    "    'CORPORATE', 'DISTRICT', 'LIMITED', 'INCORPORATED', 'PROPERTIES', 'INVESTMENT',\n",
    "    'NORTHEAST', 'PLUMBING', 'HEATING', \"T V A\", 'PARK', 'DETROIT', 'TOWNSHIP', 'CASS',\n",
    "    'ILE', 'BRANCH', 'ST', 'HCMA', 'ITC', 'TRANSMISSION', 'ENTERPRISES', 'TWP', 'FARMS', 'AUTHORITY',\n",
    "    'BERRIEN', 'CALHOUN', 'KALAMAZOO', 'CENTER', 'NATURE', 'KALAMAZOO', 'NORTHWESTERN', 'COUNTY', 'WASHTENAW'\n",
    "    ] + us_states + us_cities + company_abbr\n",
    "\n",
    "# Function to generate variations of company keywords\n",
    "def generate_variations(keyword):\n",
    "    variations = [\n",
    "        keyword,\n",
    "        keyword.lower(),\n",
    "        # keyword.upper(),\n",
    "        # keyword.capitalize(),\n",
    "    #     keyword.replace(\" \", \"\"),\n",
    "    #     keyword.replace(\" \", \".\"),\n",
    "    #     keyword.replace(\" \", \"_\"),\n",
    "    #     f\"{keyword}.\",\n",
    "    #     f\".{keyword}\",\n",
    "    #     f\"{keyword}.com\",\n",
    "    #     f\"_{keyword}_\",\n",
    "    #     f\"{keyword} Co\",\n",
    "    #     f\"{keyword} Inc.\",\n",
    "    #     f\"{keyword} LLC\",\n",
    "    #     f\"{keyword}, Inc.\",\n",
    "    #     f\"{keyword}, LLC\",\n",
    "    #     f\"{keyword}, Ltd.\",\n",
    "    #     f\"{keyword} LTD\"\n",
    "    ]\n",
    "    return variations\n",
    "\n",
    "# Augment company names using variations of keywords\n",
    "def augment_company_names(keywords, num_samples):\n",
    "    augmented_data = []\n",
    "    keyword_variations = [variation for keyword in keywords for variation in generate_variations(keyword)]\n",
    "    for _ in range(num_samples):\n",
    "        name_parts = random.sample(keyword_variations, k=1)  # Combine 3 random variations\n",
    "        company_name = \" \".join(name_parts)\n",
    "        augmented_data.append({\"Full Name\": company_name, \"Name Type\": \"Company Name\"})\n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A systematic way to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Generate synthetic data for testing\n",
    "def generate_synthetic_data(num_records=500):\n",
    "    human_first_names = [\"John\", \"Jane\", \"Alex\", \"Emily\", \"Michael\", \"Sarah\", \"David\", \"Laura\", \"James\", \"Emma\"]\n",
    "    human_last_names = [\"Smith\", \"Johnson\", \"Williams\", \"Brown\", \"Jones\", \"Garcia\", \"Miller\", \"Davis\", \"Rodriguez\", \"Martinez\"]\n",
    "    company_keywords = ['OF', 'SERVICES', 'COUNTY', 'DISTRIBUTOR', 'PRODUCTS', 'COUNTRY CLUB', 'PLLC', 'PRIVATE',\n",
    "                        'COMPANY', 'INC', 'RETIREMENT', 'DEVELOPMENT', 'HOA', 'AUTHORITY', 'CONF', 'CONFERENCE',\n",
    "                        'CONSTRUCTION', 'AFFORDABLE', 'HOUSING', 'MID COAST', 'ESTATE', 'REHABILITATION', 'GARDENS',\n",
    "                        'WELLNESS', 'LLC', 'STATE', 'LAND', 'MEMBERSHIP', 'COOPERATIVE', 'CORP', 'CORPORATION',\n",
    "                        'INDEPENDENT', 'CHURCH', 'ENTERPRISES', 'ACCOUNTING', 'INVESTMENTS', 'OWNERS', 'AIRPORT',\n",
    "                        'MAINTENANCE', 'ASSOCIATION', 'REALTY', 'FOUNDATION', 'CONSULTANTS', 'ASSOCIATES',\n",
    "                        'CORPORATE', 'DISTRICT', 'LTD', 'LIMITED', 'INCORPORATED', 'PROPERTIES', 'INVESTMENT',\n",
    "                        'NORTHEAST', 'PLUMBING', 'HEATING', \"T V A\",     'New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia', 'San Antonio', 'San Diego', 'Dallas',\n",
    "                        'San Jose', 'Austin', 'Jacksonville', 'Fort Worth', 'Columbus', 'San Francisco', 'Charlotte', 'Indianapolis', 'Seattle',\n",
    "                        'Denver', 'Washington', 'Boston', 'El Paso', 'Nashville', 'Detroit', 'Oklahoma City', 'Portland', 'Las Vegas',\n",
    "                        'Memphis', 'Louisville', 'Baltimore', 'Milwaukee', 'Albuquerque', 'Tucson', 'Fresno', 'Sacramento', 'Kansas City',\n",
    "                        'Atlanta', 'Miami', 'Colorado Springs', 'Raleigh', 'Omaha', 'Long Beach', 'Virginia Beach', 'Oakland', 'Minneapolis',\n",
    "                        'Tulsa', 'Arlington', 'Tampa', 'New Orleans', 'Wichita',\n",
    "                        'Alabama', 'AL', 'Alaska', 'AK', 'Arizona', 'AZ', 'Arkansas', 'AR', 'California', 'CA', 'Colorado', 'CO',\n",
    "                        'Connecticut', 'CT', 'Delaware', 'DE', 'Florida', 'FL', 'Georgia', 'GA', 'Hawaii', 'HI', 'Idaho', 'ID',\n",
    "                        'Illinois', 'IL', 'Indiana', 'IN', 'Iowa', 'IA', 'Kansas', 'KS', 'Kentucky', 'KY', 'Louisiana', 'LA',\n",
    "                        'Maine', 'ME', 'Maryland', 'MD', 'Massachusetts', 'MA', 'Michigan', 'MI', 'Minnesota', 'MN', 'Mississippi', 'MS',\n",
    "                        'Missouri', 'MO', 'Montana', 'MT', 'Nebraska', 'NE', 'Nevada', 'NV', 'New Hampshire', 'NH', 'New Jersey', 'NJ',\n",
    "                        'New Mexico', 'NM', 'New York', 'NY', 'North Carolina', 'NC', 'North Dakota', 'ND', 'Ohio', 'OH', 'Oklahoma', 'OK',\n",
    "                        'Oregon', 'OR', 'Pennsylvania', 'PA', 'Rhode Island', 'RI', 'South Carolina', 'SC', 'South Dakota', 'SD', 'Tennessee', 'TN',\n",
    "                        'Texas', 'TX', 'Utah', 'UT', 'Vermont', 'VT', 'Virginia', 'VA', 'Washington', 'WA', 'West Virginia', 'WV', 'Wisconsin', 'WI', 'Wyoming', 'WY',\n",
    "                        'Co', 'Inc', 'LLC', 'Ltd', 'Corp', 'Pty', 'PLC', 'GmbH', 'S.A.', 'S.A.S.', 'AG', 'N.V.', 'B.V.', 'K.K.', 'S.R.L.', 'P.C.',\n",
    "                        'C.A.', 'd.o.o.', 'P.L.C.', 'S.p.A.', 'A.G.', 'a.s.', 'OÜ', 'Oy', 'ApS', 's.r.o.', 'S.A.B.', 'S.L.', 'AB',\n",
    "                        'CASS', 'ILE', 'BRANCH', 'ST', 'HCMA', 'ITC', 'TRANSMISSION', 'ENTERPRISES']\n",
    "\n",
    "    data = []\n",
    "    for _ in range(num_records):\n",
    "        if random.random() < 0.5:\n",
    "            first_name = random.choice(human_first_names)\n",
    "            last_name = random.choice(human_last_names)\n",
    "            full_name = f\"{first_name} {last_name}\"\n",
    "            data.append({\"Full Name\": full_name, \"Name Type\": \"Human Name\"})\n",
    "        else:\n",
    "            company_name = f\"{random.choice(human_last_names)} {random.choice(company_keywords)}\"\n",
    "            data.append({\"Full Name\": company_name, \"Name Type\": \"Company Name\"})\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the Results:\n",
    "\n",
    "#### Logistic Regression on Test Set:\n",
    "\n",
    "**Accuracy on Test Set: 99.86%**\n",
    "\n",
    "**Precision, Recall, and F1-Score for Each Class:**\n",
    "\n",
    "1. **Company Name:**\n",
    "   - **Precision:** 1.00\n",
    "   - **Recall:** 1.00\n",
    "   - **F1-Score:** 1.00\n",
    "   - **Support:** 1,007,868\n",
    "\n",
    "2. **Human Name:**\n",
    "   - **Precision:** 0.98\n",
    "   - **Recall:** 0.90\n",
    "   - **F1-Score:** 0.94\n",
    "   - **Support:** 11,878\n",
    "\n",
    "**Overall Metrics:**\n",
    "   - **Accuracy:** 99.86%\n",
    "   - **Macro Average Precision:** 0.99\n",
    "   - **Macro Average Recall:** 0.95\n",
    "   - **Macro Average F1-Score:** 0.97\n",
    "   - **Weighted Average Precision:** 1.00\n",
    "   - **Weighted Average Recall:** 1.00\n",
    "   - **Weighted Average F1-Score:** 1.00\n",
    "\n",
    "**Confusion Matrix:**\n",
    "```\n",
    "[[1007662     206]\n",
    " [   1219   10659]]\n",
    "```\n",
    "- **True Negatives (Company Name correctly identified):** 1,007,662\n",
    "- **False Positives (Human Name incorrectly identified as Company Name):** 206\n",
    "- **False Negatives (Company Name incorrectly identified as Human Name):** 1,219\n",
    "- **True Positives (Human Name correctly identified):** 10,659\n",
    "\n",
    "**Analysis:**\n",
    "- The model performs exceptionally well on the test set, with an accuracy of 99.86%.\n",
    "- Precision for both classes is very high, indicating that the model is good at correctly identifying positive samples.\n",
    "- Recall for \"Human Name\" is slightly lower at 0.90, suggesting some instances of human names are missed (i.e., classified as company names).\n",
    "- F1-score is high for both classes, indicating a good balance between precision and recall.\n",
    "\n",
    "#### Logistic Regression on Synthetic Data:\n",
    "\n",
    "**Accuracy on Synthetic Data: 96.18%**\n",
    "\n",
    "**Precision, Recall, and F1-Score for Each Class:**\n",
    "\n",
    "1. **Company Name:**\n",
    "   - **Precision:** 0.99\n",
    "   - **Recall:** 0.93\n",
    "   - **F1-Score:** 0.96\n",
    "   - **Support:** 24,972\n",
    "\n",
    "2. **Human Name:**\n",
    "   - **Precision:** 0.94\n",
    "   - **Recall:** 0.99\n",
    "   - **F1-Score:** 0.96\n",
    "   - **Support:** 25,028\n",
    "\n",
    "**Overall Metrics:**\n",
    "   - **Accuracy:** 96.18%\n",
    "   - **Macro Average Precision:** 0.96\n",
    "   - **Macro Average Recall:** 0.96\n",
    "   - **Macro Average F1-Score:** 0.96\n",
    "   - **Weighted Average Precision:** 0.96\n",
    "   - **Weighted Average Recall:** 0.96\n",
    "   - **Weighted Average F1-Score:** 0.96\n",
    "\n",
    "**Confusion Matrix:**\n",
    "```\n",
    "[[23302  1670]\n",
    " [  242 24786]]\n",
    "```\n",
    "- **True Negatives (Company Name correctly identified):** 23,302\n",
    "- **False Positives (Human Name incorrectly identified as Company Name):** 1,670\n",
    "- **False Negatives (Company Name incorrectly identified as Human Name):** 242\n",
    "- **True Positives (Human Name correctly identified):** 24,786\n",
    "\n",
    "**Analysis:**\n",
    "- The model performs well on the synthetic data with an accuracy of 96.18%.\n",
    "- Precision and recall are high for both classes, indicating the model's effectiveness in distinguishing between human and company names.\n",
    "- The slight decrease in performance compared to the test set might be due to differences between the real and synthetic data or the complexity of the synthetic data.\n",
    "\n",
    "### Conclusion:\n",
    "- The logistic regression model is highly effective at distinguishing between human and company names, particularly on the test set with an accuracy of 99.86%.\n",
    "- On synthetic data, the model's accuracy drops slightly to 96.18%, but still maintains high precision, recall, and F1-scores.\n",
    "- While the recall for \"Human Name\" on the test set is slightly lower, the overall performance is strong, making the model robust for practical use in classifying human names from company names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "df = pd.read_excel(TRAINING_FILE_PATH)\n",
    "# df.drop_duplicates(inplace=True)\n",
    "df['Full Name'] = df['Full Name'].replace(',', '')\n",
    "df['Full Name'] = df['Full Name'].replace('&', '')\n",
    "\n",
    "# Separate the DataFrame into two parts\n",
    "company_df = df[df['Name Type'] == 'Company Name']\n",
    "human_df = df[df['Name Type'] == 'Human Name']\n",
    "\n",
    "# Drop duplicates in the company names part\n",
    "company_df = company_df.drop_duplicates(subset='Full Name')\n",
    "\n",
    "# Concatenate the filtered DataFrames back together\n",
    "df = pd.concat([company_df, human_df], ignore_index=True)\n",
    "\n",
    "augmented_data = augment_company_names(company_keywords, num_samples=5_000_000)\n",
    "\n",
    "# Convert augmented data to DataFrame\n",
    "augmented_df = pd.DataFrame(augmented_data)\n",
    "\n",
    "# Concatenate the original and augmented data\n",
    "df = pd.concat([df, augmented_df], ignore_index=True)\n",
    "\n",
    "synthetic_data = generate_synthetic_data(50000)\n",
    "\n",
    "# Vectorization using an improved TF-IDF approach\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 3),  # Experiment with different n-gram ranges\n",
    "    analyzer='char_wb',  # Character-level analyzer with word boundaries\n",
    "    min_df=2,  # Ignore terms that appear in less than 3 documents\n",
    "    max_df=0.65,  # Ignore terms that appear in more than 65% of the documents\n",
    "    sublinear_tf=True  # Apply sublinear term frequency scaling\n",
    ")\n",
    "X = vectorizer.fit_transform(df['Full Name'])\n",
    "y = df['Name Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg Accuracy on Test Set: 99.86%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Company Name       1.00      1.00      1.00   1007868\n",
      "  Human Name       0.98      0.90      0.94     11878\n",
      "\n",
      "    accuracy                           1.00   1019746\n",
      "   macro avg       0.99      0.95      0.97   1019746\n",
      "weighted avg       1.00      1.00      1.00   1019746\n",
      "\n",
      "[[1007662     206]\n",
      " [   1219   10659]]\n",
      "Accuracy on Synthetic Data: 96.18%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Company Name       0.99      0.93      0.96     24972\n",
      "  Human Name       0.94      0.99      0.96     25028\n",
      "\n",
      "    accuracy                           0.96     50000\n",
      "   macro avg       0.96      0.96      0.96     50000\n",
      "weighted avg       0.96      0.96      0.96     50000\n",
      "\n",
      "[[23302  1670]\n",
      " [  242 24786]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "# Define individual models with hyperparameter tuning\n",
    "logreg = LogisticRegression(max_iter=10000, penalty='l2', C=0.35, solver='newton-cg')\n",
    "\n",
    "# Train the ensemble model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = logreg.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"logreg Accuracy on Test Set: {accuracy * 100:.2f}%\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Generate and test with synthetic data\n",
    "synthetic_data = generate_synthetic_data(50000)\n",
    "synthetic_X = vectorizer.transform([row['Full Name'] for row in synthetic_data])\n",
    "synthetic_y = [row['Name Type'] for row in synthetic_data]\n",
    "\n",
    "# Evaluate on synthetic data\n",
    "synthetic_pred = logreg.predict(synthetic_X)\n",
    "synthetic_accuracy = accuracy_score(synthetic_y, synthetic_pred)\n",
    "print(f\"Accuracy on Synthetic Data: {synthetic_accuracy * 100:.2f}%\")\n",
    "print(classification_report(synthetic_y, synthetic_pred))\n",
    "print(confusion_matrix(synthetic_y, synthetic_pred))\n",
    "\n",
    "# Save the ensemble model and vectorizer\n",
    "with open(\"logreg_classifier.pickle\", \"wb\") as model_file:\n",
    "    pickle.dump(logreg, model_file)\n",
    "\n",
    "with open(\"logreg_vectorizer.pickle\", \"wb\") as vec_file:\n",
    "    pickle.dump(vectorizer, vec_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
