{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.language import Language\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('OR Crook 3-9.99.xlsx - Sheet1.csv')\n",
    "\n",
    "# Data cleaning\n",
    "df.dropna(subset=['MAIL_ADDR','MAIL_CITY','MAIL_STATE','MAIL_ZIP','OWNER_NAME_1'], inplace=True)\n",
    "df = df.drop_duplicates(['MAIL_ADDR','MAIL_CITY','MAIL_STATE','MAIL_ZIP'])\n",
    "\n",
    "# Remove commas and strip whitespace\n",
    "df['OWNER_NAME_1'] = df['OWNER_NAME_1'].str.replace(',', '').str.strip()\n",
    "\n",
    "# List of keywords indicating company names\n",
    "Suspected_Words = [\n",
    "    'OF', 'SERVICES', 'COUNTY', 'DISTRIBUTOR', 'PRODUCTS', 'COUNTRY CLUB', 'PLLC', 'PRIVATE',\n",
    "    'COMPANY', 'INC', 'RETIREMENT', 'DEVELOPMENT', 'HOA', 'AUTHORITY', 'CONF', 'CONFERENCE',\n",
    "    'CONSTRUCTION', 'AFFORDABLE', 'HOUSING', 'MID COAST', 'ESTATE', 'REHABILITATION', 'GARDENS',\n",
    "    'WELLNESS', 'LLC', 'STATE', 'LAND', 'MEMBERSHIP', 'COOPERATIVE', 'CORP', 'CORPORATION',\n",
    "    'INDEPENDENT', 'CHURCH', 'ENTERPRISES', 'ACCOUNTING', 'INVESTMENTS', 'OWNERS', 'AIRPORT',\n",
    "    'MAINTENANCE', 'ASSOCIATION', 'REALTY', 'FOUNDATION', 'CONSULTANTS', 'ASSOCIATES',\n",
    "    'CORPORATE', 'DISTRICT', 'LTD', 'LIMITED', 'INCORPORATED', 'PROPERTIES', 'INVESTMENT',\n",
    "    'NORTHEAST', 'PLUMBING', 'HEATING'\n",
    "]\n",
    "\n",
    "# Pattern to detect suspected company names\n",
    "pattern = r'\\b(' + '|'.join(Suspected_Words) + r')\\b'\n",
    "\n",
    "# Filter out suspected company names\n",
    "df = df[~df['OWNER_NAME_1'].str.contains(pattern, case=False, na=False)]\n",
    "\n",
    "# Load spaCy's pre-trained model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define custom NER component\n",
    "@Language.component(\"custom_ner\")\n",
    "def custom_ner(doc):\n",
    "    new_ents = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORG\":\n",
    "            new_ents.append(Span(doc, ent.start, ent.end, label=\"COMPANY\"))\n",
    "        else:\n",
    "            new_ents.append(ent)\n",
    "    doc.ents = new_ents\n",
    "    return doc\n",
    "\n",
    "# Add custom NER component to the pipeline\n",
    "nlp.add_pipe(\"custom_ner\", last=True)\n",
    "\n",
    "# Define a function to classify names\n",
    "def classify_name(name):\n",
    "    try:\n",
    "        doc = nlp(name)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"PERSON\":\n",
    "                return \"Human Name\"\n",
    "            elif ent.label_ == \"COMPANY\":\n",
    "                return \"Company Name\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing name '{name}': {e}\")\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Apply the classification function to the \"OWNER_NAME_1\" column\n",
    "df[\"Name Type\"] = df[\"OWNER_NAME_1\"].apply(classify_name)\n",
    "\n",
    "# Save the results to a new CSV\n",
    "df.to_csv('Removed_keywords_datasheet.csv', index=False)\n",
    "print(\"Classification complete. Results saved to 'Removed_keywords_datasheet.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The rest of this notebook is still not done and ***Can't be Used***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['OWNER_NAME_1'][df['Name Type'] == 'Company Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('OR Crook 3-9.99.xlsx - Sheet1.csv')\n",
    "df.dropna(subset=['MAIL_ADDR','MAIL_CITY','MAIL_STATE','MAIL_ZIP','OWNER_NAME_1'], inplace=True)\n",
    "df = df.drop_duplicates(['MAIL_ADDR','MAIL_CITY','MAIL_STATE','MAIL_ZIP'])\n",
    "\n",
    "df['OWNER_NAME_1'] = df['OWNER_NAME_1'].replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['OWNER_NAME_1'].apply(lambda x: x.replace(',', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import os\n",
    "\n",
    "def load_training_data(file_path, sheet_name='Training Data'):\n",
    "    \"\"\"\n",
    "    Load training data from an Excel file and prepare it for training.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the Excel file.\n",
    "        sheet_name (str): Sheet name in the Excel file containing the training data.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of training examples formatted for spaCy.\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    train_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = row['Full Name']\n",
    "        label = row['Name Type']\n",
    "        if label == \"Human Name\":\n",
    "            entities = [(0, len(text), \"PERSON\")]\n",
    "        elif label == \"Company Name\":\n",
    "            entities = [(0, len(text), \"ORG\")]\n",
    "        train_data.append((text, {\"entities\": entities}))\n",
    "    return train_data\n",
    "\n",
    "def train_model(nlp, train_data, val_data, n_iter=10, dropout=0.5):\n",
    "    \"\"\"\n",
    "    Train the NER model.\n",
    "\n",
    "    Args:\n",
    "        nlp (Language): The spaCy language model.\n",
    "        train_data (list): List of training data.\n",
    "        val_data (list): List of validation data.\n",
    "        n_iter (int): Number of training iterations.\n",
    "        dropout (float): Dropout rate.\n",
    "\n",
    "    \"\"\"\n",
    "    pipe_exceptions = [\"ner\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.resume_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                examples = []\n",
    "                for text, annotations in batch:\n",
    "                    doc = nlp.make_doc(text)\n",
    "                    example = Example.from_dict(doc, annotations)\n",
    "                    examples.append(example)\n",
    "                nlp.update(examples, drop=dropout, losses=losses)\n",
    "            print(f\"Iteration {itn + 1}/{n_iter}, Losses: {losses}\")\n",
    "            evaluate_model(nlp, val_data)\n",
    "\n",
    "def evaluate_model(nlp, val_data):\n",
    "    \"\"\"\n",
    "    Evaluate the NER model on validation data.\n",
    "\n",
    "    Args:\n",
    "        nlp (Language): The spaCy language model.\n",
    "        val_data (list): List of validation data.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for text, annotations in val_data:\n",
    "        doc = nlp(text)\n",
    "        ents = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        true_ents = annotations['entities']\n",
    "        if ents == true_ents:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    accuracy = correct / total\n",
    "    print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and prepare training data\n",
    "    train_data = load_training_data('Training Data Scrubbing Names.xlsx')\n",
    "    # Remove commas and strip whitespace\n",
    "    df['Full Name'] = df['Full Name'].str.replace(',', '').str.strip()\n",
    "    train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Load pre-trained spaCy model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Add NER pipe if not present\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe(\"ner\")\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # Add new labels\n",
    "    ner.add_label(\"PERSON\")\n",
    "    ner.add_label(\"ORG\")\n",
    "\n",
    "    # Train the model\n",
    "    train_model(nlp, train_data, val_data, n_iter=20)\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model_directory = \"path/to/save/custom_ner_model\"\n",
    "    os.makedirs(model_directory, exist_ok=True)\n",
    "    nlp.to_disk(model_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Saving the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Your training and model preparation code here...\n",
    "\n",
    "# Directory to save the model\n",
    "model_directory = \"custom_ner_model\"\n",
    "\n",
    "# Train the model (assuming the train_model function from previous example)\n",
    "train_model(nlp, train_data, val_data, n_iter=20)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(model_directory, exist_ok=True)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "nlp.to_disk(model_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model_directory = \"path/to/save/custom_ner_model\"\n",
    "nlp = spacy.load(model_directory)\n",
    "\n",
    "# Define a function to classify names\n",
    "def classify_name(name):\n",
    "    doc = nlp(name)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            return \"Human Name\"\n",
    "        elif ent.label_ == \"ORG\":\n",
    "            return \"Company Name\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Test the model with new data\n",
    "test_names = [\"John Doe\", \"Tech Innovators LLC\", \"Acme Corporation\"]\n",
    "for name in test_names:\n",
    "    name_type = classify_name(name)\n",
    "    print(f\"{name}: {name_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name Type\n",
       "Human Name      0.87619\n",
       "Company Name    0.12381\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"Training Data Scrubbing Names.xlsx\")\n",
    "\n",
    "df['Name Type'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
