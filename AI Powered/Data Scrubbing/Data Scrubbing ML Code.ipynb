{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "TRAINING_FILE_PATH = \"Training Data Scrubbing Names 3.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of U.S. states and their abbreviations\n",
    "us_states = [\n",
    "    'Alabama', 'AL', 'Alaska', 'AK', 'Arizona', 'AZ', 'Arkansas', 'AR', 'California', 'CA', 'Colorado', 'CO',\n",
    "    'Connecticut', 'CT', 'Delaware', 'DE', 'Florida', 'FL', 'Georgia', 'GA', 'Hawaii', 'HI', 'Idaho', 'ID',\n",
    "    'Illinois', 'IL', 'Indiana', 'IN', 'Iowa', 'IA', 'Kansas', 'KS', 'Kentucky', 'KY', 'Louisiana', 'LA',\n",
    "    'Maine', 'ME', 'Maryland', 'MD', 'Massachusetts', 'MA', 'Michigan', 'MI', 'Minnesota', 'MN', 'Mississippi', 'MS',\n",
    "    'Missouri', 'MO', 'Montana', 'MT', 'Nebraska', 'NE', 'Nevada', 'NV', 'New Hampshire', 'NH', 'New Jersey', 'NJ',\n",
    "    'New Mexico', 'NM', 'New York', 'NY', 'North Carolina', 'NC', 'North Dakota', 'ND', 'Ohio', 'OH', 'Oklahoma', 'OK',\n",
    "    'Oregon', 'OR', 'Pennsylvania', 'PA', 'Rhode Island', 'RI', 'South Carolina', 'SC', 'South Dakota', 'SD', 'Tennessee', 'TN',\n",
    "    'Texas', 'TX', 'Utah', 'UT', 'Vermont', 'VT', 'Virginia', 'VA', 'Washington', 'WA', 'West Virginia', 'WV', 'Wisconsin', 'WI', 'Wyoming', 'WY'\n",
    "]\n",
    "\n",
    "# List of major U.S. cities\n",
    "us_cities = [\n",
    "    'New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia', 'San Antonio', 'San Diego', 'Dallas',\n",
    "    'San Jose', 'Austin', 'Jacksonville', 'Fort Worth', 'Columbus', 'San Francisco', 'Charlotte', 'Indianapolis', 'Seattle',\n",
    "    'Denver', 'Washington', 'Boston', 'El Paso', 'Nashville', 'Detroit', 'Oklahoma City', 'Portland', 'Las Vegas',\n",
    "    'Memphis', 'Louisville', 'Baltimore', 'Milwaukee', 'Albuquerque', 'Tucson', 'Fresno', 'Sacramento', 'Kansas City',\n",
    "    'Atlanta', 'Miami', 'Colorado Springs', 'Raleigh', 'Omaha', 'Long Beach', 'Virginia Beach', 'Oakland', 'Minneapolis',\n",
    "    'Tulsa', 'Arlington', 'Tampa', 'New Orleans', 'Wichita'\n",
    "]\n",
    "\n",
    "# List of common company-related abbreviations\n",
    "company_abbr = [\n",
    "    'Co', 'Inc', 'LLC', 'Ltd', 'Corp', 'Pty', 'PLC', 'GmbH', 'S.A.', 'S.A.S.', 'AG', 'N.V.', 'B.V.', 'K.K.', 'S.R.L.', 'P.C.',\n",
    "    'C.A.', 'd.o.o.', 'P.L.C.', 'S.p.A.', 'A.G.', 'a.s.', 'OÃœ', 'Oy', 'ApS', 's.r.o.', 'S.A.B.', 'S.L.', 'AB'\n",
    "]\n",
    "\n",
    "# Combine all keywords\n",
    "company_keywords = [\n",
    "    'OF', 'SERVICES', 'COUNTY', 'DISTRIBUTOR', 'PRODUCTS', 'COUNTRY CLUB', 'PLLC', 'PRIVATE',\n",
    "    'COMPANY', 'RETIREMENT', 'DEVELOPMENT', 'HOA', 'AUTHORITY', 'CONF', 'CONFERENCE',\n",
    "    'CONSTRUCTION', 'AFFORDABLE', 'HOUSING', 'MID COAST', 'ESTATE', 'REHABILITATION', 'GARDENS',\n",
    "    'WELLNESS', 'STATE', 'LAND', 'MEMBERSHIP', 'COOPERATIVE', 'CORPORATION',\n",
    "    'INDEPENDENT', 'CHURCH', 'ENTERPRISES', 'ACCOUNTING', 'INVESTMENTS', 'OWNERS', 'AIRPORT',\n",
    "    'MAINTENANCE', 'ASSOCIATION', 'REALTY', 'FOUNDATION', 'CONSULTANTS', 'ASSOCIATES',\n",
    "    'CORPORATE', 'DISTRICT', 'LIMITED', 'INCORPORATED', 'PROPERTIES', 'INVESTMENT',\n",
    "    'NORTHEAST', 'PLUMBING', 'HEATING', \"T V A\"\n",
    "] + us_states + us_cities + company_abbr\n",
    "\n",
    "# Function to generate variations of company keywords\n",
    "def generate_variations(keyword):\n",
    "    variations = [\n",
    "        keyword,\n",
    "        keyword.lower(),\n",
    "        keyword.upper(),\n",
    "        keyword.capitalize(),\n",
    "        keyword.replace(\" \", \"\"),\n",
    "        keyword.replace(\" \", \".\"),\n",
    "        keyword.replace(\" \", \"_\"),\n",
    "        f\"{keyword}.\",\n",
    "        f\".{keyword}\",\n",
    "        f\"{keyword}.com\",\n",
    "        f\"_{keyword}_\",\n",
    "        f\"{keyword} Co\",\n",
    "        f\"{keyword} Inc.\",\n",
    "        f\"{keyword} LLC\",\n",
    "        f\"{keyword}, Inc.\",\n",
    "        f\"{keyword}, LLC\",\n",
    "        f\"{keyword}, Ltd.\",\n",
    "        f\"{keyword} LTD\"\n",
    "    ]\n",
    "    return variations\n",
    "\n",
    "# Augment company names using variations of keywords\n",
    "def augment_company_names(keywords, num_samples):\n",
    "    augmented_data = []\n",
    "    keyword_variations = [variation for keyword in keywords for variation in generate_variations(keyword)]\n",
    "    for _ in range(num_samples):\n",
    "        name_parts = random.sample(keyword_variations, k=3)  # Combine 3 random variations\n",
    "        company_name = \" \".join(name_parts)\n",
    "        augmented_data.append({\"Full Name\": company_name, \"Name Type\": \"Company Name\"})\n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation:\n",
    "- High Precision for Human Names: The model is very precise in identifying human names, with few false positives.\n",
    "- High Recall for Company Names: The model correctly identifies most company names, missing only a small percentage.\n",
    "- Lower Recall for Human Names: The model is less effective at identifying all human names, as indicated by the lower recall.\n",
    "\n",
    "- ***Overall, the model performs well with a balanced F1-score of 0.94 for both classes. However, it is slightly better at classifying company names (higher recall) than human names. Further tuning, such as optimizing the decision threshold or incorporating more features, could improve performance.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A systematic way to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Generate synthetic data for testing\n",
    "def generate_synthetic_data(num_records=500):\n",
    "    human_first_names = [\"John\", \"Jane\", \"Alex\", \"Emily\", \"Michael\", \"Sarah\", \"David\", \"Laura\", \"James\", \"Emma\"]\n",
    "    human_last_names = [\"Smith\", \"Johnson\", \"Williams\", \"Brown\", \"Jones\", \"Garcia\", \"Miller\", \"Davis\", \"Rodriguez\", \"Martinez\"]\n",
    "    company_keywords = ['OF', 'SERVICES', 'COUNTY', 'DISTRIBUTOR', 'PRODUCTS', 'COUNTRY CLUB', 'PLLC', 'PRIVATE', \n",
    "                        'COMPANY', 'INC', 'RETIREMENT', 'DEVELOPMENT', 'HOA', 'AUTHORITY', 'CONF', 'CONFERENCE', \n",
    "                        'CONSTRUCTION', 'AFFORDABLE', 'HOUSING', 'MID COAST', 'ESTATE', 'REHABILITATION', 'GARDENS', \n",
    "                        'WELLNESS', 'LLC', 'STATE', 'LAND', 'MEMBERSHIP', 'COOPERATIVE', 'CORP', 'CORPORATION', \n",
    "                        'INDEPENDENT', 'CHURCH', 'ENTERPRISES', 'ACCOUNTING', 'INVESTMENTS', 'OWNERS', 'AIRPORT', \n",
    "                        'MAINTENANCE', 'ASSOCIATION', 'REALTY', 'FOUNDATION', 'CONSULTANTS', 'ASSOCIATES', \n",
    "                        'CORPORATE', 'DISTRICT', 'LTD', 'LIMITED', 'INCORPORATED', 'PROPERTIES', 'INVESTMENT', \n",
    "                        'NORTHEAST', 'PLUMBING', 'HEATING', \"T V A\"]\n",
    "\n",
    "    data = []\n",
    "    for _ in range(num_records):\n",
    "        if random.random() < 0.5:\n",
    "            first_name = random.choice(human_first_names)\n",
    "            last_name = random.choice(human_last_names)\n",
    "            full_name = f\"{first_name} {last_name}\"\n",
    "            data.append({\"Full Name\": full_name, \"Name Type\": \"Human Name\"})\n",
    "        else:\n",
    "            company_name = f\"{random.choice(human_last_names)} {random.choice(company_keywords)}\"\n",
    "            data.append({\"Full Name\": company_name, \"Name Type\": \"Company Name\"})\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Models Comparison to see which models are best performing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN: CV Accuracy: 0.98 (+/- 0.001)\n",
      "Accuracy on Synthetic Data: 88.35%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Company Name       0.85      0.93      0.89      5029\n",
      "  Human Name       0.92      0.84      0.88      4971\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.89      0.88      0.88     10000\n",
      "weighted avg       0.89      0.88      0.88     10000\n",
      "\n",
      "[[4669  360]\n",
      " [ 805 4166]]\n",
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "df = pd.read_excel(TRAINING_FILE_PATH)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df['Full Name'] = df['Full Name'].replace(',', '')\n",
    "df['Full Name'] = df['Full Name'].replace('&', '')\n",
    "\n",
    "\n",
    "augmented_data = augment_company_names(company_keywords, num_samples=20000)\n",
    "\n",
    "# Convert augmented data to DataFrame\n",
    "augmented_df = pd.DataFrame(augmented_data)\n",
    "\n",
    "# Concatenate the original and augmented data\n",
    "df = pd.concat([df, augmented_df], ignore_index=True)\n",
    "\n",
    "synthetic_data = generate_synthetic_data(10000)\n",
    "\n",
    "# Vectorization using an improved TF-IDF approach\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 3),  # Experiment with different n-gram ranges\n",
    "    analyzer='char_wb',  # Character-level analyzer with word boundaries\n",
    "    min_df=3,  # Ignore terms that appear in less than 3 documents\n",
    "    max_df=0.7,  # Ignore terms that appear in more than 70% of the documents\n",
    "    sublinear_tf=True  # Apply sublinear term frequency scaling\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(df['Full Name'])\n",
    "y = df['Name Type']\n",
    "\n",
    "# Prepare synthetic data for evaluation\n",
    "synthetic_X = vectorizer.transform([row['Full Name'] for row in synthetic_data])\n",
    "synthetic_y = [row['Name Type'] for row in synthetic_data]\n",
    "\n",
    "# Models to evaluate\n",
    "models = [\n",
    "    ('KNN', KNeighborsClassifier()),\n",
    "    ('SVC', SVC()),\n",
    "    ('DecisionTree', DecisionTreeClassifier()),\n",
    "    ('RandomForest', RandomForestClassifier()),\n",
    "    ('AdaBoost', AdaBoostClassifier()),\n",
    "    ('GradientBoost', GradientBoostingClassifier()),\n",
    "    ('GaussianNB', GaussianNB()),\n",
    "    ('LDA', LinearDiscriminantAnalysis()),\n",
    "    ('LogisticRegression', LogisticRegression(max_iter=5000)),\n",
    "    ('MLP', MLPClassifier(max_iter=500))\n",
    "]\n",
    "\n",
    "# Evaluate each model with KFold cross-validation\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "\n",
    "    # Convert to dense array for models that require it\n",
    "    if name in ['GaussianNB', 'LDA']:\n",
    "        X_dense = X.toarray()\n",
    "        synthetic_X_dense = synthetic_X.toarray()\n",
    "        cv_results = cross_val_score(model, X_dense, y, cv=kfold, scoring=scoring)\n",
    "    else:\n",
    "        cv_results = cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n",
    "\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print(f\"{name}: CV Accuracy: {np.round(cv_results.mean(), 3)} (+/- {np.round(cv_results.std(), 3)})\")\n",
    "\n",
    "    # Train model on the entire dataset and evaluate on synthetic data\n",
    "    model.fit(X_dense if name in ['GaussianNB', 'LDA'] else X, y)\n",
    "    synthetic_pred = model.predict(synthetic_X_dense if name in ['GaussianNB', 'LDA'] else synthetic_X)\n",
    "    synthetic_accuracy = accuracy_score(synthetic_y, synthetic_pred)\n",
    "    print(f\"Accuracy on Synthetic Data: {synthetic_accuracy * 100:.2f}%\")\n",
    "    print(classification_report(synthetic_y, synthetic_pred))\n",
    "    print(confusion_matrix(synthetic_y, synthetic_pred))\n",
    "    print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "# Boxplot algorithm comparison\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.savefig('Models_Comparison.png', format='png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using Gridsearch to get the best parameters for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for SVC: {'C': 5, 'gamma': 'scale', 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid_svc = {\n",
    "    'C': [0.1, 0.5, 1, 5],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "grid_svc = GridSearchCV(SVC(), param_grid_svc, cv=5, scoring='accuracy')\n",
    "grid_svc.fit(X_train, y_train)\n",
    "best_svc = grid_svc.best_estimator_\n",
    "print(\"Best parameters for SVC:\", grid_svc.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=5, scoring='accuracy')\n",
    "grid_rf.fit(X_train, y_train)\n",
    "best_rf = grid_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", grid_rf.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Enhancing the model with States, Cities and company abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on Test Set: 98.12%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Company Name       0.99      0.97      0.98      8945\n",
      "  Human Name       0.97      0.99      0.98      7295\n",
      "\n",
      "    accuracy                           0.98     16240\n",
      "   macro avg       0.98      0.98      0.98     16240\n",
      "weighted avg       0.98      0.98      0.98     16240\n",
      "\n",
      "[[8689  256]\n",
      " [  49 7246]]\n",
      "Accuracy on Synthetic Data: 85.60%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Company Name       1.00      0.71      0.83      4893\n",
      "  Human Name       0.78      1.00      0.88      5107\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.89      0.85      0.85     10000\n",
      "weighted avg       0.89      0.86      0.85     10000\n",
      "\n",
      "[[3453 1440]\n",
      " [   0 5107]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_excel(TRAINING_FILE_PATH)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df['Full Name'] = df['Full Name'].replace(',', '')\n",
    "df['Full Name'] = df['Full Name'].replace('&', '')\n",
    "\n",
    "augmented_data = augment_company_names(company_keywords, num_samples=20000)\n",
    "\n",
    "# Convert augmented data to DataFrame\n",
    "augmented_df = pd.DataFrame(augmented_data)\n",
    "\n",
    "# Concatenate the original and augmented data\n",
    "df = pd.concat([df, augmented_df], ignore_index=True)\n",
    "\n",
    "# Vectorization using an improved TF-IDF approach\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 3),  # Experiment with different n-gram ranges\n",
    "    analyzer='char_wb',  # Character-level analyzer with word boundaries\n",
    "    min_df=3,  # Ignore terms that appear in less than 3 documents\n",
    "    max_df=0.7,  # Ignore terms that appear in more than 70% of the documents\n",
    "    sublinear_tf=True  # Apply sublinear term frequency scaling\n",
    ")\n",
    "X = vectorizer.fit_transform(df['Full Name'])\n",
    "y = df['Name Type']\n",
    "\n",
    "# Shuffle and split the data into training and test sets\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X, y = X[indices], y.iloc[indices]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "# Define individual models with hyperparameter tuning\n",
    "logreg = LogisticRegression(max_iter=5000, penalty='l2', C=0.35, solver='newton-cg')\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(500,), max_iter=2000, alpha=0.001, solver='adam')\n",
    "svc = SVC(C=5, gamma='scale', kernel='rbf', probability=True)\n",
    "rf = RandomForestClassifier(bootstrap=False, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200)\n",
    "gb = GradientBoostingClassifier(n_estimators=300, random_state=42)\n",
    "\n",
    "# Create an ensemble model using StackingClassifier for advanced blending\n",
    "ensemble_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('logreg', logreg),\n",
    "        ('mlp', mlp),\n",
    "        ('svc', svc),\n",
    "        ('rf', rf)\n",
    "    ],\n",
    "    final_estimator=gb,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Train the ensemble model\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Ensemble Model Accuracy on Test Set: {accuracy * 100:.2f}%\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Generate and test with synthetic data\n",
    "synthetic_data = generate_synthetic_data(10000)\n",
    "synthetic_X = vectorizer.transform([row['Full Name'] for row in synthetic_data])\n",
    "synthetic_y = [row['Name Type'] for row in synthetic_data]\n",
    "\n",
    "# Evaluate on synthetic data\n",
    "synthetic_pred = ensemble_model.predict(synthetic_X)\n",
    "synthetic_accuracy = accuracy_score(synthetic_y, synthetic_pred)\n",
    "print(f\"Accuracy on Synthetic Data: {synthetic_accuracy * 100:.2f}%\")\n",
    "print(classification_report(synthetic_y, synthetic_pred))\n",
    "print(confusion_matrix(synthetic_y, synthetic_pred))\n",
    "\n",
    "# Save the ensemble model and vectorizer\n",
    "with open(\"enhanced_ensemble_name_classifier.pickle\", \"wb\") as model_file:\n",
    "    pickle.dump(ensemble_model, model_file)\n",
    "\n",
    "with open(\"vectorizer.pickle\", \"wb\") as vec_file:\n",
    "    pickle.dump(vectorizer, vec_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Saving the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vectorizer\n",
    "with open(\"tfidf_vectorizer.pickle\", \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "# Save the model\n",
    "with open(\"ensemble_model.pickle\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model \n",
    "model = pickle.load(open('name_classifier_logreg1.pickle', 'rb'))\n",
    "\n",
    "\n",
    "# Generate and test with synthetic data\n",
    "synthetic_data = generate_synthetic_data(10000)\n",
    "synthetic_X = vectorizer.transform([row['Full Name'] for row in synthetic_data])\n",
    "synthetic_y = [row['Name Type'] for row in synthetic_data]\n",
    "# Evaluate on synthetic data\n",
    "synthetic_pred = model.predict(synthetic_X)\n",
    "synthetic_accuracy = accuracy_score(synthetic_y, synthetic_pred)\n",
    "print(f\"Accuracy on Synthetic Data: {synthetic_accuracy * 100:.2f}%\")\n",
    "print(classification_report(synthetic_y, synthetic_pred))\n",
    "print(confusion_matrix(synthetic_y, synthetic_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Different ways of to enhnce the bagging strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Accuracy on Test Set: 99.05%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Company Name       1.00      0.99      0.99      8958\n",
      "  Human Name       0.98      1.00      0.99      7282\n",
      "\n",
      "    accuracy                           0.99     16240\n",
      "   macro avg       0.99      0.99      0.99     16240\n",
      "weighted avg       0.99      0.99      0.99     16240\n",
      "\n",
      "[[8824  134]\n",
      " [  21 7261]]\n",
      "Accuracy on Synthetic Data: 84.97%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Company Name       1.00      0.69      0.82      4893\n",
      "  Human Name       0.77      1.00      0.87      5107\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.89      0.85      0.85     10000\n",
      "weighted avg       0.88      0.85      0.85     10000\n",
      "\n",
      "[[3390 1503]\n",
      " [   0 5107]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_excel(TRAINING_FILE_PATH)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df['Full Name'] = df['Full Name'].replace(',', '')\n",
    "df['Full Name'] = df['Full Name'].replace('&', '')\n",
    "\n",
    "augmented_data = augment_company_names(company_keywords, num_samples=20000)\n",
    "\n",
    "# Convert augmented data to DataFrame\n",
    "augmented_df = pd.DataFrame(augmented_data)\n",
    "\n",
    "# Concatenate the original and augmented data\n",
    "df = pd.concat([df, augmented_df], ignore_index=True)\n",
    "\n",
    "# Vectoxrization using TF-IDF\n",
    "vectorxizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='char')\n",
    "X = vectorizer.fit_transform(df['Full Name'])\n",
    "y = df['Name Type']\n",
    "\n",
    "# Shuffle and split the data into training and test sets\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X, y = X[indices], y.iloc[indices]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "# Define individual models\n",
    "logreg = LogisticRegression(max_iter=5000, penalty='l2', C=0.35, solver='newton-cg')\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, alpha=0.001, solver='adam', random_state=42)\n",
    "svc = SVC(probability=True, kernel='linear', C=1.0, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create an ensemble model\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('logreg', logreg),\n",
    "        ('mlp', mlp),\n",
    "        ('svc', svc),\n",
    "        ('rf', rf)\n",
    "    ],\n",
    "    voting='soft'  # Use 'soft' voting to consider probabilities\n",
    ")\n",
    "\n",
    "# Train the ensemble model\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Ensemble Model Accuracy on Test Set: {accuracy * 100:.2f}%\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Generate and test with synthetic data\n",
    "synthetic_data = generate_synthetic_data(10000)\n",
    "synthetic_X = vectorizer.transform([row['Full Name'] for row in synthetic_data])\n",
    "synthetic_y = [row['Name Type'] for row in synthetic_data]\n",
    "\n",
    "# Evaluate on synthetic data\n",
    "synthetic_pred = ensemble_model.predict(synthetic_X)\n",
    "synthetic_accuracy = accuracy_score(synthetic_y, synthetic_pred)\n",
    "print(f\"Accuracy on Synthetic Data: {synthetic_accuracy * 100:.2f}%\")\n",
    "print(classification_report(synthetic_y, synthetic_pred))\n",
    "print(confusion_matrix(synthetic_y, synthetic_pred))\n",
    "\n",
    "# Save the ensemble model and vectorizer\n",
    "with open(\"ensemble_name_classifier.pickle\", \"wb\") as model_file:\n",
    "    pickle.dump(ensemble_model, model_file)\n",
    "\n",
    "with open(\"vectorizer.pickle\", \"wb\") as vec_file:\n",
    "    pickle.dump(vectorizer, vec_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ak758\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "c:\\Users\\ak758\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_excel(TRAINING_FILE_PATH)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df['Full Name'] = df['Full Name'].replace(',', '')\n",
    "df['Full Name'] = df['Full Name'].replace('&', '')\n",
    "\n",
    "augmented_data = augment_company_names(company_keywords, num_samples=20000)\n",
    "\n",
    "# Convert augmented data to DataFrame\n",
    "augmented_df = pd.DataFrame(augmented_data)\n",
    "\n",
    "# Concatenate the original and augmented data\n",
    "df = pd.concat([df, augmented_df], ignore_index=True)\n",
    "\n",
    "# Vectorization using TF-IDF\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='char')\n",
    "X = vectorizer.fit_transform(df['Full Name'])\n",
    "y = df['Name Type']\n",
    "\n",
    "# Shuffle and split the data into training and test sets\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X, y = X[indices], y.iloc[indices]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "# Define individual models with hyperparameter tuning\n",
    "logreg = LogisticRegression(max_iter=5000, penalty='l2', C=0.35, solver='newton-cg')\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(500,), max_iter=2000, alpha=0.001, solver='adam')\n",
    "svc = SVC(C=5, gamma='scale', kernel='rbf', probability=True)\n",
    "rf = RandomForestClassifier(bootstrap=False, max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200)\n",
    "gb = GradientBoostingClassifier(n_estimators=300, random_state=42)\n",
    "\n",
    "# Create an ensemble model using StackingClassifier for advanced blending\n",
    "ensemble_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('logreg', logreg),\n",
    "        ('mlp', mlp),\n",
    "        ('svc', svc),\n",
    "        ('rf', rf)\n",
    "    ],\n",
    "    final_estimator=gb,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Train the ensemble model\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Ensemble Model Accuracy on Test Set: {accuracy * 100:.2f}%\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Generate and test with synthetic data\n",
    "synthetic_data = generate_synthetic_data(10000)\n",
    "synthetic_X = vectorizer.transform([row['Full Name'] for row in synthetic_data])\n",
    "synthetic_y = [row['Name Type'] for row in synthetic_data]\n",
    "\n",
    "# Evaluate on synthetic data\n",
    "synthetic_pred = ensemble_model.predict(synthetic_X)\n",
    "synthetic_accuracy = accuracy_score(synthetic_y, synthetic_pred)\n",
    "print(f\"Accuracy on Synthetic Data: {synthetic_accuracy * 100:.2f}%\")\n",
    "print(classification_report(synthetic_y, synthetic_pred))\n",
    "print(confusion_matrix(synthetic_y, synthetic_pred))\n",
    "\n",
    "# Save the ensemble model and vectorizer\n",
    "with open(\"enhanced_ensemble_name_classifier.pickle\", \"wb\") as model_file:\n",
    "    pickle.dump(ensemble_model, model_file)\n",
    "\n",
    "with open(\"vectorizer.pickle\", \"wb\") as vec_file:\n",
    "    pickle.dump(vectorizer, vec_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
