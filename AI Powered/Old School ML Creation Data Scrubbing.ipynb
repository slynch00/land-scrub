{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import os\n",
    "\n",
    "def load_training_data(file_path):\n",
    "    \"\"\"\n",
    "    Load training data from an Excel file and prepare it for training.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the Excel file.\n",
    "        sheet_name (str): Sheet name in the Excel file containing the training data.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of training examples formatted for spaCy.\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(file_path)\n",
    "    # Remove commas and strip whitespace\n",
    "    df['Full Name'] = df['Full Name'].str.replace(',', '').str.strip()\n",
    "    train_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = row['Full Name']\n",
    "        label = row['Name Type']\n",
    "        if label == \"Human Name\":\n",
    "            entities = [(0, len(text), \"PERSON\")]\n",
    "        elif label == \"Company Name\":\n",
    "            entities = [(0, len(text), \"ORG\")]\n",
    "        train_data.append((text, {\"entities\": entities}))\n",
    "    return train_data\n",
    "\n",
    "def train_model(nlp, train_data, val_data, n_iter=10, dropout=0.5):\n",
    "    \"\"\"\n",
    "    Train the NER model.\n",
    "\n",
    "    Args:\n",
    "        nlp (Language): The spaCy language model.\n",
    "        train_data (list): List of training data.\n",
    "        val_data (list): List of validation data.\n",
    "        n_iter (int): Number of training iterations.\n",
    "        dropout (float): Dropout rate.\n",
    "\n",
    "    \"\"\"\n",
    "    pipe_exceptions = [\"ner\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.resume_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                examples = []\n",
    "                for text, annotations in batch:\n",
    "                    doc = nlp.make_doc(text)\n",
    "                    example = Example.from_dict(doc, annotations)\n",
    "                    examples.append(example)\n",
    "                nlp.update(examples, drop=dropout, losses=losses)\n",
    "            print(f\"Iteration {itn + 1}/{n_iter}, Losses: {losses}\")\n",
    "            evaluate_model(nlp, val_data)\n",
    "\n",
    "def evaluate_model(nlp, val_data):\n",
    "    \"\"\"\n",
    "    Evaluate the NER model on validation data.\n",
    "\n",
    "    Args:\n",
    "        nlp (Language): The spaCy language model.\n",
    "        val_data (list): List of validation data.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for text, annotations in val_data:\n",
    "        doc = nlp(text)\n",
    "        ents = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        true_ents = annotations['entities']\n",
    "        if ents == true_ents:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    accuracy = correct / total\n",
    "    print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and prepare training data\n",
    "    train_data = load_training_data('Training Data Scrubbing Names.xlsx')\n",
    "    \n",
    "    train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Load pre-trained spaCy model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Add NER pipe if not present\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe(\"ner\")\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # Add new labels\n",
    "    ner.add_label(\"PERSON\")\n",
    "    ner.add_label(\"ORG\")\n",
    "\n",
    "    # Train the model\n",
    "    train_model(nlp, train_data, val_data, n_iter=20)\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model_directory = \"custom_ner_model\"\n",
    "    os.makedirs(model_directory, exist_ok=True)\n",
    "    nlp.to_disk(model_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Handling Imbalanced Data Using Under-Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.language import Language\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import random\n",
    "\n",
    "# Load the training data\n",
    "df = pd.read_excel('Training Data Scrubbing Names.xlsx')\n",
    "# Remove commas and strip whitespace\n",
    "df['Full Name'] = df['Full Name'].str.replace(',', '').str.strip()\n",
    "df['Full Name'] = df['Full Name'].str.replace('&', '').str.strip()\n",
    "\n",
    "# # Separate majority and minority classes\n",
    "# df_human = df[df['Name Type'] == 'Human Name']\n",
    "# df_company = df[df['Name Type'] == 'Company Name']\n",
    "\n",
    "# # Under-sample the majority class\n",
    "# df_human_under = resample(df_human,\n",
    "#                           replace=False,  # sample without replacement\n",
    "#                           n_samples=len(df_company),  # match minority class\n",
    "#                           random_state=42)  # reproducible results\n",
    "\n",
    "# # Combine minority class with downsampled majority class\n",
    "# df_balanced = pd.concat([df_human_under, df_company])\n",
    "\n",
    "# # Shuffle the dataset\n",
    "# df_balanced = df_balanced.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Prepare training data\n",
    "TRAIN_DATA = []\n",
    "for _, row in df.iterrows():\n",
    "    text = row['Full Name']\n",
    "    label = row['Name Type']\n",
    "    if label == \"Human Name\":\n",
    "        entities = [(0, len(text), \"PERSON\")]\n",
    "    elif label == \"Company Name\":\n",
    "        entities = [(0, len(text), \"ORG\")]\n",
    "    TRAIN_DATA.append((text, {\"entities\": entities}))\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_data, val_data = train_test_split(TRAIN_DATA, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Load the pre-trained spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add a new pipe to the model (if needed)\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\")\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Add labels to the NER component\n",
    "ner.add_label(\"PERSON\")\n",
    "ner.add_label(\"ORG\")\n",
    "\n",
    "# Disable other pipes during training\n",
    "pipe_exceptions = [\"ner\"]\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "def train_model(nlp, train_data, val_data, n_iter=10, dropout=0.3):\n",
    "    \"\"\"Train the NER model.\"\"\"\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.resume_training()\n",
    "        for itn in range(n_iter):  # number of iterations\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            # Batch the examples and shuffle\n",
    "            batches = minibatch(train_data, size=compounding(4.0, 64.0, 1.001))\n",
    "            for batch in batches:\n",
    "                examples = []\n",
    "                for text, annotations in batch:\n",
    "                    doc = nlp.make_doc(text)\n",
    "                    example = Example.from_dict(doc, annotations)\n",
    "                    examples.append(example)\n",
    "                nlp.update(examples, drop=dropout, losses=losses)\n",
    "            print(f\"Iteration {itn + 1}/{n_iter}, Losses: {losses}\")\n",
    "            evaluate_model(nlp, val_data)  # Evaluate on validation set\n",
    "\n",
    "def evaluate_model(nlp, val_data):\n",
    "    \"\"\"Evaluate the model on validation data.\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for text, annotations in val_data:\n",
    "        doc = nlp(text)\n",
    "        ents = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        true_ents = annotations['entities']\n",
    "        if ents == true_ents:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    accuracy = correct / total\n",
    "    print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Train the model\n",
    "train_model(nlp, train_data, val_data, n_iter=20)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "nlp.to_disk(\"custom_ner_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simple Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model_directory = \"custom_ner_model\"\n",
    "nlp = spacy.load(model_directory)\n",
    "\n",
    "# Define a function to classify names\n",
    "def classify_name(name):\n",
    "    doc = nlp(name)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            return \"Human Name\"\n",
    "        elif ent.label_ == \"ORG\":\n",
    "            return \"Company Name\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Test the model with new data\n",
    "test_names = [\"Brown ASSOC\", \"Tech Innovators LLC\", \"Acme Corporation\", \"Miller FOR\", 'Martinez  COUNTRY CLUB']\n",
    "for name in test_names:\n",
    "    name_type = classify_name(name)\n",
    "    print(f\"{name}: {name_type}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Enhanced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import random\n",
    "\n",
    "# Load the training data\n",
    "df = pd.read_excel('Training Data Scrubbing Names.xlsx')\n",
    "# Remove commas and strip whitespace\n",
    "df['Full Name'] = df['Full Name'].str.replace(',', '').str.strip()\n",
    "df['Full Name'] = df['Full Name'].str.replace('&', '').str.strip()\n",
    "\n",
    "# Prepare training data\n",
    "TRAIN_DATA = []\n",
    "for _, row in df.iterrows():\n",
    "    text = row['Full Name']\n",
    "    label = row['Name Type']\n",
    "    if label == \"Human Name\":\n",
    "        entities = [(0, len(text), \"PERSON\")]\n",
    "    elif label == \"Company Name\":\n",
    "        entities = [(0, len(text), \"ORG\")]\n",
    "    TRAIN_DATA.append((text, {\"entities\": entities}))\n",
    "\n",
    "# Handle class imbalance with under-sampling\n",
    "def balance_classes(data, ratio=1.0):\n",
    "    human_names = [item for item in data if item[1]['entities'][0][2] == \"PERSON\"]\n",
    "    company_names = [item for item in data if item[1]['entities'][0][2] == \"ORG\"]\n",
    "    if len(human_names) > len(company_names):\n",
    "        human_names = random.sample(human_names, int(len(company_names) * ratio))\n",
    "    else:\n",
    "        company_names = random.sample(company_names, int(len(human_names) / ratio))\n",
    "    return human_names + company_names\n",
    "\n",
    "# Balance and shuffle the training data\n",
    "# TRAIN_DATA = balance_classes(TRAIN_DATA)\n",
    "random.shuffle(TRAIN_DATA)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_data, val_data = train_test_split(TRAIN_DATA, test_size=0.3, random_state=42)\n",
    "\n",
    "# Load the pre-trained spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add or get the NER component in the pipeline\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\")\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Add labels to the NER component\n",
    "ner.add_label(\"PERSON\")\n",
    "ner.add_label(\"ORG\")\n",
    "\n",
    "# Disable other pipes during training\n",
    "pipe_exceptions = [\"ner\"]\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "# Data augmentation function\n",
    "def augment_data(data, multiplier=1.2):\n",
    "    augmented_data = []\n",
    "    for text, annotations in data:\n",
    "        augmented_data.append((text, annotations))\n",
    "        if annotations['entities'][0][2] == \"PERSON\":\n",
    "            augmented_data.append((text.lower(), annotations))\n",
    "        elif annotations['entities'][0][2] == \"ORG\":\n",
    "            augmented_data.append((text.upper(), annotations))\n",
    "    random.shuffle(augmented_data)\n",
    "    return augmented_data[:int(len(augmented_data) * multiplier)]\n",
    "\n",
    "# Augment the training data\n",
    "train_data = augment_data(train_data)\n",
    "\n",
    "# Training function with logging and early stopping\n",
    "def train_model(nlp, train_data, val_data, n_iter=30, dropout=0.5):\n",
    "    best_f1 = 0\n",
    "    no_improve_epochs = 0\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.resume_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                examples = []\n",
    "                for text, annotations in batch:\n",
    "                    doc = nlp.make_doc(text)\n",
    "                    example = Example.from_dict(doc, annotations)\n",
    "                    examples.append(example)\n",
    "                nlp.update(examples, drop=dropout, losses=losses)\n",
    "            val_f1 = evaluate_model(nlp, val_data)\n",
    "            print(f\"Iteration {itn + 1}/{n_iter}, Losses: {losses}, Validation F1-Score: {val_f1:.2f}\")\n",
    "\n",
    "            if val_f1 > best_f1:\n",
    "                best_f1 = val_f1\n",
    "                no_improve_epochs = 0\n",
    "            else:\n",
    "                no_improve_epochs += 1\n",
    "\n",
    "            if no_improve_epochs > 5:  # No improvement for 5 epochs\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "# Evaluation function using F1 score\n",
    "def evaluate_model(nlp, val_data):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for text, annotations in val_data:\n",
    "        doc = nlp(text)\n",
    "        ents = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        true_ents = annotations['entities']\n",
    "        y_true.append(true_ents[0][2])\n",
    "        y_pred.append(ents[0][2] if ents else \"O\")\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    print(f\"Classification Report:\\n{classification_report(y_true, y_pred, labels=['PERSON', 'ORG'])}\")\n",
    "    return f1\n",
    "\n",
    "# Train the model\n",
    "train_model(nlp, train_data, val_data, n_iter=20, dropout=0.3)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "nlp.to_disk(\"custom_ner_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Generating synthetic data for testing\n",
    "def generate_synthetic_data(num_records=200):\n",
    "    # Lists of common human first and last names and company keywords\n",
    "    human_first_names = [\"John\", \"Jane\", \"Alex\", \"Emily\", \"Michael\", \"Sarah\", \"David\", \"Laura\", \"James\", \"Emma\"]\n",
    "    human_last_names = [\"Smith\", \"Johnson\", \"Williams\", \"Brown\", \"Jones\", \"Garcia\", \"Miller\", \"Davis\", \"Rodriguez\", \"Martinez\"]\n",
    "    \n",
    "    # List of keywords indicating company names\n",
    "    company_keywords = ['OF', 'SERVICES','COUNTY','DISTRIBUTOR','PRODUCTS',' COUNTRY CLUB','PLLC','PRIVATE''COMPANY', 'INC','RETIREMENT','DEVELOPMENT','HOA','AUTHORITY','CONF','CONFERENCE','CONSTRCTN','AFFORDABLE','HSNG','MID COAST','ESTATE','REHABILITATION','GARDENS','WELLNESS'\n",
    "                    'LLC', 'STATE', 'LAND','MEMBERSHIP','COOPERATIVE' 'CORP','CORPORATION','INDEPENDENT','ARCARE','CHURCH','ENTERPRISES','ACCOUNTING','INVESTMENTS','OWNERS','AIRPORT','MAINTENANCE','MOTORCYCLE','ASSOC','EXCAVATING','EXCAVATION','PROPS','RECREATIONAL','VILLAGE','FOR','GROUND','REAL'\n",
    "                    'CO', 'REALTY', 'CONSTRUCTION', 'FOUNDATION', 'CONSULTANTS', 'ASSOCIATES', 'CORPORATE','DISTRICT','LTD','LIMITED','INCORPORATED','PROPERTIES','INVESTMENT','ASSN','TRAILS','NORTHEAST','ESTS','RIVERGATE','PLUMBING','HEATING']\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for _ in range(num_records):\n",
    "        # Randomly decide to create a human name or company name\n",
    "        if random.random() < 0.5:\n",
    "            # Create a human name\n",
    "            first_name = random.choice(human_first_names)\n",
    "            last_name = random.choice(human_last_names)\n",
    "            full_name = f\"{first_name} {last_name}\"\n",
    "            data.append({\"Full Name\": full_name, \"Name Type\": \"Human Name\"})\n",
    "        else:\n",
    "            # Create a company name\n",
    "            company_name = f\"{random.choice(human_last_names)} {random.choice(company_keywords)}\"\n",
    "            data.append({\"Full Name\": company_name, \"Name Type\": \"Company Name\"})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Generate 200 records\n",
    "synthetic_data = generate_synthetic_data(200)\n",
    "\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load the fine-tuned model\n",
    "nlp = spacy.load(\"custom_ner_model\")\n",
    "\n",
    "# Function to classify names using the model\n",
    "def classify_name(name):\n",
    "    doc = nlp(name)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            return \"Human Name\"\n",
    "        elif ent.label_ == \"COMPANY\":\n",
    "            return \"Company Name\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Evaluate the model on synthetic data\n",
    "correct = 0\n",
    "total = len(synthetic_data)\n",
    "\n",
    "for record in synthetic_data:\n",
    "    predicted = classify_name(record['Full Name'])\n",
    "    actual = record['Name Type']\n",
    "    if predicted == actual:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Model Accuracy on Synthetic Data: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the training data\n",
    "df = pd.read_excel('Training Data Scrubbing Names 2.xlsx')\n",
    "\n",
    "# Prepare training data\n",
    "TRAIN_DATA = []\n",
    "for _, row in df.iterrows():\n",
    "    text = row['Full Name']\n",
    "    label = row['Name Type']\n",
    "    if label == \"Human Name\":\n",
    "        entities = [(0, len(text), \"PERSON\")]\n",
    "    elif label == \"Company Name\":\n",
    "        entities = [(0, len(text), \"ORG\")]\n",
    "    TRAIN_DATA.append((text, {\"entities\": entities}))\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_data, val_data = train_test_split(TRAIN_DATA, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load a pre-trained spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")  # Use 'en_core_web_lg' for more robust embeddings if needed\n",
    "\n",
    "# Check if NER pipe exists, else create it\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\")\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Add labels to the NER component\n",
    "ner.add_label(\"PERSON\")\n",
    "ner.add_label(\"ORG\")\n",
    "\n",
    "# Disable other pipes during training\n",
    "pipe_exceptions = [\"ner\"]\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "# Augmentation Function\n",
    "def augment_data(data):\n",
    "    augmented_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        # Simple augmentation: case variations and minor spelling changes\n",
    "        if entities[0][2] == \"PERSON\":\n",
    "            augmented_texts = [text.upper(), text.lower(), text[::-1]]  # Reverse string as a naive augmentation\n",
    "        elif entities[0][2] == \"ORG\":\n",
    "            augmented_texts = [text.replace(\"Inc\", \"Inc.\"), text.replace(\"LLC\", \"L.L.C.\")]\n",
    "        for aug_text in augmented_texts:\n",
    "            augmented_data.append((aug_text, annotations))\n",
    "    return data + augmented_data\n",
    "\n",
    "# Augment the training data\n",
    "train_data = augment_data(train_data)\n",
    "\n",
    "# Custom training function with additional strategies\n",
    "def train_model(nlp, train_data, val_data, n_iter=20, dropout=0.3):\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.resume_training()\n",
    "        optimizer.learn_rate = 0.001  # Hyperparameter tuning\n",
    "        for itn in range(n_iter):  # number of iterations\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            # Batch the examples and shuffle\n",
    "            batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                examples = []\n",
    "                for text, annotations in batch:\n",
    "                    doc = nlp.make_doc(text)\n",
    "                    example = Example.from_dict(doc, annotations)\n",
    "                    examples.append(example)\n",
    "                nlp.update(examples, drop=dropout, losses=losses, sgd=optimizer)\n",
    "            print(f\"Iteration {itn + 1}/{n_iter}, Losses: {losses}\")\n",
    "            evaluate_model(nlp, val_data)  # Evaluate on validation set\n",
    "\n",
    "# Evaluation function with classification report\n",
    "def evaluate_model(nlp, val_data):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for text, annotations in val_data:\n",
    "        doc = nlp(text)\n",
    "        ents = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        true_ents = annotations['entities']\n",
    "        y_true.append(true_ents[0][2] if true_ents else \"None\")\n",
    "        y_pred.append(ents[0][2] if ents else \"None\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Train the model\n",
    "train_model(nlp, train_data, val_data, n_iter=20)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "nlp.to_disk(\"custom_ner_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Method: Example of using the same model with different initializations (optional)\n",
    "def ensemble_predict(text, models):\n",
    "    preds = []\n",
    "    for model in models:\n",
    "        doc = model(text)\n",
    "        ents = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        preds.append(ents[0][2] if ents else \"None\")\n",
    "    # Majority voting\n",
    "    final_pred = max(set(preds), key=preds.count)\n",
    "    return final_pred\n",
    "\n",
    "# Load models and test ensemble (if multiple models are available)\n",
    "models = [nlp]  # Extend this list with other model paths if needed\n",
    "text = \"John Steve\"\n",
    "predicted_label = ensemble_predict(text, models)\n",
    "print(f\"Ensemble predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ensemble Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the training data\n",
    "df = pd.read_excel('Training Data Scrubbing Names 2.xlsx')\n",
    "\n",
    "# Prepare training data\n",
    "TRAIN_DATA = []\n",
    "for _, row in df.iterrows():\n",
    "    text = row['Full Name']\n",
    "    label = row['Name Type']\n",
    "    if label == \"Human Name\":\n",
    "        entities = [(0, len(text), \"PERSON\")]\n",
    "    elif label == \"Company Name\":\n",
    "        entities = [(0, len(text), \"ORG\")]\n",
    "    TRAIN_DATA.append((text, {\"entities\": entities}))\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_data, val_data = train_test_split(TRAIN_DATA, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load spaCy models\n",
    "models = {\n",
    "    \"sm\": spacy.load(\"en_core_web_sm\"),\n",
    "    \"md\": spacy.load(\"en_core_web_md\"),\n",
    "    \"lg\": spacy.load(\"en_core_web_lg\")\n",
    "}\n",
    "\n",
    "# Add a new pipe to each model if needed and add labels\n",
    "for model_key in models:\n",
    "    nlp = models[model_key]\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe(\"ner\")\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "    ner.add_label(\"PERSON\")\n",
    "    ner.add_label(\"ORG\")\n",
    "\n",
    "# Custom training function with additional strategies\n",
    "def train_model(nlp, train_data, val_data, n_iter=20, dropout=0.3):\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.resume_training()\n",
    "        optimizer.learn_rate = 0.001  # Hyperparameter tuning\n",
    "        for itn in range(n_iter):  # number of iterations\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                examples = []\n",
    "                for text, annotations in batch:\n",
    "                    doc = nlp.make_doc(text)\n",
    "                    example = Example.from_dict(doc, annotations)\n",
    "                    examples.append(example)\n",
    "                nlp.update(examples, drop=dropout, losses=losses, sgd=optimizer)\n",
    "            print(f\"Iteration {itn + 1}/{n_iter}, Losses: {losses}\")\n",
    "            evaluate_model(nlp, val_data)  # Evaluate on validation set\n",
    "\n",
    "# Evaluation function with classification report\n",
    "def evaluate_model(nlp, val_data):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for text, annotations in val_data:\n",
    "        doc = nlp(text)\n",
    "        ents = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        true_ents = annotations['entities']\n",
    "        y_true.append(true_ents[0][2] if true_ents else \"None\")\n",
    "        y_pred.append(ents[0][2] if ents else \"None\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Train each model\n",
    "for model_key in models:\n",
    "    print(f\"Training model: {model_key}\")\n",
    "    train_model(models[model_key], train_data, val_data, n_iter=20)\n",
    "\n",
    "# Save the fine-tuned models\n",
    "for model_key, model in models.items():\n",
    "    model.to_disk(f\"{model_key}_ner_model\")\n",
    "\n",
    "# Ensemble prediction method\n",
    "def ensemble_predict(text, models):\n",
    "    preds = []\n",
    "    for model in models.values():\n",
    "        doc = model(text)\n",
    "        ents = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        preds.append(ents[0][2] if ents else \"None\")\n",
    "    final_pred = max(set(preds), key=preds.count)\n",
    "    return final_pred\n",
    "\n",
    "# Test ensemble method\n",
    "test_text = \"John Doe\"\n",
    "predicted_label = ensemble_predict(test_text, models)\n",
    "print(f\"Ensemble predicted label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Training Data Scrubbing Names 2.xlsx'\n",
    "data = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "# Convert dict_keys to a list to access the first sheet\n",
    "sheet_names = list(data.keys())\n",
    "sheet_names, data[sheet_names[0]].head()\n",
    "\n",
    "# Load spaCy modelsz\n",
    "nlp_lg = spacy.load('en_core_web_lg')\n",
    "nlp_md = spacy.load('en_core_web_md')\n",
    "nlp_sm = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to extract features using spaCy models\n",
    "def extract_features(text, model):\n",
    "    doc = model(text)\n",
    "    return doc.vector\n",
    "\n",
    "# Extracting features for each spaCy model\n",
    "data = data[sheet_names[0]]\n",
    "X_lg = np.array([extract_features(name, nlp_lg) for name in data['Full Name']])\n",
    "X_md = np.array([extract_features(name, nlp_md) for name in data['Full Name']])\n",
    "X_sm = np.array([extract_features(name, nlp_sm) for name in data['Full Name']])\n",
    "\n",
    "# Target variable\n",
    "y = data['Name Type'].apply(lambda x: 1 if x == 'Human Name' else 0).values\n",
    "\n",
    "# Train-test split\n",
    "X_train_lg, X_test_lg, y_train, y_test = train_test_split(X_lg, y, test_size=0.2, random_state=42)\n",
    "X_train_md, X_test_md = train_test_split(X_md, test_size=0.2, random_state=42)\n",
    "X_train_sm, X_test_sm = train_test_split(X_sm, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model training\n",
    "clf_lg = RandomForestClassifier(random_state=42)\n",
    "clf_md = RandomForestClassifier(random_state=42)\n",
    "clf_sm = RandomForestClassifier(random_state=42)\n",
    "\n",
    "clf_lg.fit(X_train_lg, y_train)\n",
    "clf_md.fit(X_train_md, y_train)\n",
    "clf_sm.fit(X_train_sm, y_train)\n",
    "\n",
    "# Predictions\n",
    "pred_lg = clf_lg.predict(X_test_lg)\n",
    "pred_md = clf_md.predict(X_test_md)\n",
    "pred_sm = clf_sm.predict(X_test_sm)\n",
    "\n",
    "# Ensemble: Averaging predictions\n",
    "ensemble_pred = (pred_lg + pred_md + pred_sm) / 3\n",
    "ensemble_pred = np.round(ensemble_pred).astype(int)\n",
    "\n",
    "# Accuracy\n",
    "accuracy_lg = accuracy_score(y_test, pred_lg)\n",
    "accuracy_md = accuracy_score(y_test, pred_md)\n",
    "accuracy_sm = accuracy_score(y_test, pred_sm)\n",
    "accuracy_ensemble = accuracy_score(y_test, ensemble_pred)\n",
    "\n",
    "accuracy_lg, accuracy_md, accuracy_sm, accuracy_ensemble\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Full Name      | Name Type   |\n",
      "|:---------------|:------------|\n",
      "| MAIZE, MIKE    | Human Name  |\n",
      "| ANDREWS JR     | Human Name  |\n",
      "| REEL, STEVEN   | Human Name  |\n",
      "| GOFF, CODY     | Human Name  |\n",
      "| BOMAR, MICHAEL | Human Name  |\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30203 entries, 0 to 30202\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Full Name  30203 non-null  object\n",
      " 1   Name Type  30203 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 472.1+ KB\n",
      "None\n",
      "Logistic Regression: Mean CV Score: 0.7521, Std Dev: 0.0055\n",
      "Logistic Regression: Test Accuracy: 0.7560\n",
      "\n",
      "Random Forest: Mean CV Score: 0.8027, Std Dev: 0.0048\n",
      "Random Forest: Test Accuracy: 0.8091\n",
      "\n",
      "SVC: Mean CV Score: 0.7930, Std Dev: 0.0045\n",
      "SVC: Test Accuracy: 0.7927\n",
      "\n",
      "The best model is Random Forest with a test accuracy of 0.8091\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_excel('Training Data Scrubbing Names 2.xlsx')\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "# Print the column names and their data types\n",
    "print(df.info())\n",
    "\n",
    "# Drop null values in `Full Name`\n",
    "df.dropna(subset=['Full Name'], inplace=True)\n",
    "\n",
    "# Convert to categorical\n",
    "df['Name Type'] = df['Name Type'].astype('category')\n",
    "\n",
    "# Create a copy of the dataframe for feature engineering\n",
    "df_model = df.copy()\n",
    "\n",
    "# Create new features\n",
    "df_model['name_length'] = df_model['Full Name'].apply(len)\n",
    "df_model['has_title'] = df_model['Full Name'].str.contains('Mr\\.|Ms\\.|Dr\\.', regex=True).astype(int)\n",
    "df_model['has_suffix'] = df_model['Full Name'].str.contains('Inc\\.|LLC|Corp', regex=True).astype(int)\n",
    "df_model['num_words'] = df_model['Full Name'].str.split().apply(len)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X = df_model[['name_length', 'has_title', 'has_suffix', 'num_words']]\n",
    "y = df_model['Name Type']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'SVC': SVC(random_state=42)\n",
    "}\n",
    "\n",
    "# Cross-validation and training\n",
    "for name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    print(f\"{name}: Mean CV Score: {np.mean(cv_scores):.4f}, Std Dev: {np.std(cv_scores):.4f}\")\n",
    "\n",
    "    # Fit on the entire training set\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate and print test accuracy\n",
    "    test_accuracy = (y_pred == y_test).mean()\n",
    "    print(f\"{name}: Test Accuracy: {test_accuracy:.4f}\\n\")\n",
    "\n",
    "# Determine the best model\n",
    "best_model = max(models, key=lambda name: models[name].score(X_test, y_test))\n",
    "best_accuracy = models[best_model].score(X_test, y_test)\n",
    "\n",
    "if all(models[model].score(X_test, y_test) == best_accuracy for model in models):\n",
    "    print(\"No clear best model. Further analysis or model tuning may be needed.\")\n",
    "else:\n",
    "    print(f\"The best model is {best_model} with a test accuracy of {best_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
